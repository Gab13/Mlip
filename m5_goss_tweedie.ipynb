{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from  datetime import datetime, timedelta\n",
    "import gc\n",
    "import numpy as np, pandas as pd\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAL_DTYPES = {\n",
    "    \"event_name_1\": \"category\", \n",
    "    \"event_name_2\": \"category\", \n",
    "    \"event_type_1\": \"category\", \n",
    "    \"event_type_2\": \"category\", \n",
    "    \"weekday\": \"category\", \n",
    "    'wm_yr_wk': 'int16', \n",
    "    \"wday\": \"int16\",\n",
    "    \"month\": \"int16\", \n",
    "    \"year\": \"int16\", \n",
    "    \"snap_CA\": \"float32\", \n",
    "    'snap_TX': 'float32', \n",
    "    'snap_WI': 'float32' }\n",
    "PRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices_path = \"./sell_prices.csv\"\n",
    "cal_path = \"./calendar.csv\"\n",
    "train_path = \"./sales_train_validation.csv\"\n",
    "\n",
    "h = 28 \n",
    "max_lags = 57\n",
    "tr_last = 1913  # Last day of training data\n",
    "fday = datetime(2016,4, 25)  # Last date of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(is_train = True, nrows = None, first_day = 1200):\n",
    "    # Read price data and transform datatypes\n",
    "    prices = pd.read_csv(prices_path, dtype = PRICE_DTYPES)\n",
    "    for col, col_dtype in PRICE_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            prices[col] = prices[col].cat.codes.astype(\"int16\")\n",
    "            prices[col] -= prices[col].min()\n",
    "    print(prices.shape)\n",
    "    \n",
    "    # Read calendar data and transform datatypes\n",
    "    cal = pd.read_csv(cal_path, dtype = CAL_DTYPES)\n",
    "    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n",
    "    for col, col_dtype in CAL_DTYPES.items():\n",
    "        if col_dtype == \"category\":\n",
    "            cal[col] = cal[col].cat.codes.astype(\"int16\")\n",
    "            cal[col] -= cal[col].min()\n",
    "    print(cal.shape)\n",
    "    \n",
    "    # First day to use\n",
    "    start_day = max(1 if is_train  else tr_last-max_lags, first_day)\n",
    "    \n",
    "    numcols = [f\"d_{day}\" for day in range(start_day,tr_last+1)]\n",
    "    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n",
    "    dtype = {numcol:\"float32\" for numcol in numcols} \n",
    "    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n",
    "    \n",
    "    # Read training data abd transform datatypes\n",
    "    dt = pd.read_csv(train_path, nrows = nrows, usecols = catcols + numcols, dtype = dtype)\n",
    "    for col in catcols:\n",
    "        if col != \"id\":\n",
    "            dt[col] = dt[col].cat.codes.astype(\"int16\")\n",
    "            dt[col] -= dt[col].min()\n",
    "    print(dt.shape)\n",
    "    \n",
    "    if not is_train:\n",
    "        for day in range(tr_last+1, tr_last+ 28 +1):\n",
    "            dt[f\"d_{day}\"] = np.nan\n",
    "    \n",
    "    # Transform horizontal data (day columns) to vertical data (1 row per product per day)\n",
    "    dt = pd.melt(dt,\n",
    "                  id_vars = catcols,\n",
    "                  value_vars = [col for col in dt.columns if col.startswith(\"d_\")],\n",
    "                  var_name = \"d\",\n",
    "                  value_name = \"sales\")\n",
    "    \n",
    "    # Merge training data with prices and calendar\n",
    "    dt = dt.merge(cal, on= \"d\", copy = False)\n",
    "    dt = dt.merge(prices, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n",
    "    \n",
    "    return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(dt):\n",
    "    \n",
    "    # Defining the lag features\n",
    "    lags = [7, 28]\n",
    "    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n",
    "    for lag, lag_col in zip(lags, lag_cols):\n",
    "        dt[lag_col] = dt[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n",
    "\n",
    "    # Defining rolling mean features on lags\n",
    "    wins = [7, 28]\n",
    "    for win in wins :\n",
    "        for lag,lag_col in zip(lags, lag_cols):\n",
    "            dt[f\"rmean_{lag}_{win}\"] = dt[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n",
    "\n",
    "    # Transform data information\n",
    "    date_features = {\n",
    "        \n",
    "        \"wday\": \"weekday\",\n",
    "        \"week\": \"weekofyear\",\n",
    "        \"month\": \"month\",\n",
    "        \"quarter\": \"quarter\",\n",
    "        \"year\": \"year\",\n",
    "        \"mday\": \"day\",\n",
    "    }\n",
    "    \n",
    "    for date_feat_name, date_feat_func in date_features.items():\n",
    "        if date_feat_name in dt.columns:\n",
    "            dt[date_feat_name] = dt[date_feat_name].astype(\"int16\")\n",
    "        else:\n",
    "            dt[date_feat_name] = getattr(dt[\"date\"].dt, date_feat_func).astype(\"int16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FIRST_DAY = 800 # If you want to load all the data set it to '1' -->  Great  memory overflow  risk !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6841121, 4)\n",
      "(1969, 14)\n",
      "(30490, 1120)\n",
      "CPU times: user 19.6 s, sys: 5.83 s, total: 25.4 s\n",
      "Wall time: 25.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31522396, 22)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = pre_process(is_train=True, first_day= FIRST_DAY)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 35s, sys: 17.7 s, total: 2min 52s\n",
      "Wall time: 2min 52s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(31522396, 31)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "build_features(df)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29845446, 31)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows with missing values\n",
    "df.dropna(inplace = True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_feats = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + [\"event_name_1\", \"event_name_2\", \"event_type_1\", \"event_type_2\"]\n",
    "useless_cols = [\"id\", \"date\", \"sales\",\"d\", \"wm_yr_wk\", \"weekday\"]  # Columns not used for training\n",
    "train_cols = df.columns[~df.columns.isin(useless_cols)]\n",
    "X_train = df[train_cols]  # Training set\n",
    "y_train = df[\"sales\"]  # Ground truth labels\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29845446, 25)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14.8 s, sys: 2.02 s, total: 16.8 s\n",
      "Wall time: 16.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "np.random.seed(777)\n",
    "\n",
    "fake_valid_inds = np.random.choice(X_train.index.values, 2_000_000, replace = False)\n",
    "train_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)\n",
    "train_data = lgb.Dataset(X_train.loc[train_inds] , label = y_train.loc[train_inds], \n",
    "                         categorical_feature=cat_feats, free_raw_data=False)\n",
    "fake_valid_data = lgb.Dataset(X_train.loc[fake_valid_inds], label = y_train.loc[fake_valid_inds],\n",
    "                              categorical_feature=cat_feats,\n",
    "                 free_raw_data=False)# This is a random sample, we're not gonna apply any time series train-test-split tricks here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del X_train, y_train, fake_valid_inds,train_inds ; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM parameters\n",
    "params = {\n",
    "        \"boosting\": \"goss\",\n",
    "        \"objective\" : \"tweedie\",\n",
    "        \"tweedie_variance_power\": 1.1,\n",
    "        \"metric\" :\"rmse\",\n",
    "        \"force_row_wise\" : True,\n",
    "        \"learning_rate\" : 0.05,\n",
    "        #\"sub_row\" : 0.5,\n",
    "        #\"bagging_freq\" : 1,\n",
    "        \"lambda_l2\" : 0.1,\n",
    "        \"metric\": \"rmse\",\n",
    "        \"verbosity\": 1,\n",
    "        \"num_iterations\": 1400,\n",
    "        \"num_leaves\": 128,\n",
    "        \"min_data_in_leaf\": 100,\n",
    "        \"feature_fraction\": 0.6,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\tvalid_0's rmse: 2.77269\n",
      "[40]\tvalid_0's rmse: 2.42385\n",
      "[60]\tvalid_0's rmse: 2.35279\n",
      "[80]\tvalid_0's rmse: 2.33247\n",
      "[100]\tvalid_0's rmse: 2.32144\n",
      "[120]\tvalid_0's rmse: 2.31421\n",
      "[140]\tvalid_0's rmse: 2.30749\n",
      "[160]\tvalid_0's rmse: 2.30226\n",
      "[180]\tvalid_0's rmse: 2.29561\n",
      "[200]\tvalid_0's rmse: 2.29073\n",
      "[220]\tvalid_0's rmse: 2.28528\n",
      "[240]\tvalid_0's rmse: 2.28289\n",
      "[260]\tvalid_0's rmse: 2.27964\n",
      "[280]\tvalid_0's rmse: 2.27676\n",
      "[300]\tvalid_0's rmse: 2.27278\n",
      "[320]\tvalid_0's rmse: 2.27073\n",
      "[340]\tvalid_0's rmse: 2.26691\n",
      "[360]\tvalid_0's rmse: 2.26395\n",
      "[380]\tvalid_0's rmse: 2.26141\n",
      "[400]\tvalid_0's rmse: 2.25837\n",
      "[420]\tvalid_0's rmse: 2.25682\n",
      "[440]\tvalid_0's rmse: 2.25447\n",
      "[460]\tvalid_0's rmse: 2.25241\n",
      "[480]\tvalid_0's rmse: 2.25062\n",
      "[500]\tvalid_0's rmse: 2.24868\n",
      "[520]\tvalid_0's rmse: 2.24783\n",
      "[540]\tvalid_0's rmse: 2.24601\n",
      "[560]\tvalid_0's rmse: 2.24471\n",
      "[580]\tvalid_0's rmse: 2.24356\n",
      "[600]\tvalid_0's rmse: 2.24196\n",
      "[620]\tvalid_0's rmse: 2.24121\n",
      "[640]\tvalid_0's rmse: 2.24075\n",
      "[660]\tvalid_0's rmse: 2.23959\n",
      "[680]\tvalid_0's rmse: 2.23864\n",
      "[700]\tvalid_0's rmse: 2.23717\n",
      "[720]\tvalid_0's rmse: 2.23617\n",
      "[740]\tvalid_0's rmse: 2.23572\n",
      "[760]\tvalid_0's rmse: 2.23528\n",
      "[780]\tvalid_0's rmse: 2.23479\n",
      "[800]\tvalid_0's rmse: 2.23415\n",
      "[820]\tvalid_0's rmse: 2.23375\n",
      "[840]\tvalid_0's rmse: 2.23348\n",
      "[860]\tvalid_0's rmse: 2.23297\n",
      "[1020]\tvalid_0's rmse: 2.22832\n",
      "[1040]\tvalid_0's rmse: 2.22792\n",
      "[1060]\tvalid_0's rmse: 2.22724\n",
      "[1080]\tvalid_0's rmse: 2.22687\n",
      "[1100]\tvalid_0's rmse: 2.22676\n",
      "[1120]\tvalid_0's rmse: 2.22561\n",
      "[1140]\tvalid_0's rmse: 2.22525\n",
      "[1160]\tvalid_0's rmse: 2.22464\n",
      "[1180]\tvalid_0's rmse: 2.22435\n",
      "[1200]\tvalid_0's rmse: 2.22402\n",
      "[1220]\tvalid_0's rmse: 2.22382\n",
      "[1240]\tvalid_0's rmse: 2.22373\n",
      "[1260]\tvalid_0's rmse: 2.22362\n",
      "[1280]\tvalid_0's rmse: 2.22318\n",
      "[1300]\tvalid_0's rmse: 2.22294\n",
      "[1320]\tvalid_0's rmse: 2.22253\n",
      "[1340]\tvalid_0's rmse: 2.22224\n",
      "[1360]\tvalid_0's rmse: 2.22187\n",
      "[1380]\tvalid_0's rmse: 2.22187\n",
      "[1400]\tvalid_0's rmse: 2.2217\n",
      "CPU times: user 7h 25min 27s, sys: 1min 5s, total: 7h 26min 33s\n",
      "Wall time: 1h 52min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7f395039d590>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Training a model with LightGBM\n",
    "m_lgb = lgb.train(params, train_data, valid_sets = [fake_valid_data], verbose_eval=20) \n",
    "m_lgb.save_model(\"model.lgb\")\n",
    "# m_lgb = lgb.Booster(model_file='model.lgb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6841121, 4)\n",
      "(1969, 14)\n",
      "(30490, 64)\n",
      "0 2016-04-25 00:00:00\n",
      "1 2016-04-26 00:00:00\n",
      "2 2016-04-27 00:00:00\n",
      "3 2016-04-28 00:00:00\n",
      "4 2016-04-29 00:00:00\n",
      "5 2016-04-30 00:00:00\n",
      "6 2016-05-01 00:00:00\n",
      "7 2016-05-02 00:00:00\n",
      "8 2016-05-03 00:00:00\n",
      "9 2016-05-04 00:00:00\n",
      "10 2016-05-05 00:00:00\n",
      "11 2016-05-06 00:00:00\n",
      "12 2016-05-07 00:00:00\n",
      "13 2016-05-08 00:00:00\n",
      "14 2016-05-09 00:00:00\n",
      "15 2016-05-10 00:00:00\n",
      "16 2016-05-11 00:00:00\n",
      "17 2016-05-12 00:00:00\n",
      "18 2016-05-13 00:00:00\n",
      "19 2016-05-14 00:00:00\n",
      "20 2016-05-15 00:00:00\n",
      "21 2016-05-16 00:00:00\n",
      "22 2016-05-17 00:00:00\n",
      "23 2016-05-18 00:00:00\n",
      "24 2016-05-19 00:00:00\n",
      "25 2016-05-20 00:00:00\n",
      "26 2016-05-21 00:00:00\n",
      "27 2016-05-22 00:00:00\n",
      "0 1.028 0.3333333333333333\n",
      "(6841121, 4)\n",
      "(1969, 14)\n",
      "(30490, 64)\n",
      "0 2016-04-25 00:00:00\n",
      "1 2016-04-26 00:00:00\n",
      "2 2016-04-27 00:00:00\n",
      "3 2016-04-28 00:00:00\n",
      "4 2016-04-29 00:00:00\n",
      "5 2016-04-30 00:00:00\n",
      "6 2016-05-01 00:00:00\n",
      "7 2016-05-02 00:00:00\n",
      "8 2016-05-03 00:00:00\n",
      "9 2016-05-04 00:00:00\n",
      "10 2016-05-05 00:00:00\n",
      "11 2016-05-06 00:00:00\n",
      "12 2016-05-07 00:00:00\n",
      "13 2016-05-08 00:00:00\n",
      "14 2016-05-09 00:00:00\n",
      "15 2016-05-10 00:00:00\n",
      "16 2016-05-11 00:00:00\n",
      "17 2016-05-12 00:00:00\n",
      "18 2016-05-13 00:00:00\n",
      "19 2016-05-14 00:00:00\n",
      "20 2016-05-15 00:00:00\n",
      "21 2016-05-16 00:00:00\n",
      "22 2016-05-17 00:00:00\n",
      "23 2016-05-18 00:00:00\n",
      "24 2016-05-19 00:00:00\n",
      "25 2016-05-20 00:00:00\n",
      "26 2016-05-21 00:00:00\n",
      "27 2016-05-22 00:00:00\n",
      "1 1.023 0.3333333333333333\n",
      "(6841121, 4)\n",
      "(1969, 14)\n",
      "(30490, 64)\n",
      "0 2016-04-25 00:00:00\n",
      "1 2016-04-26 00:00:00\n",
      "2 2016-04-27 00:00:00\n",
      "3 2016-04-28 00:00:00\n",
      "4 2016-04-29 00:00:00\n",
      "5 2016-04-30 00:00:00\n",
      "6 2016-05-01 00:00:00\n",
      "7 2016-05-02 00:00:00\n",
      "8 2016-05-03 00:00:00\n",
      "9 2016-05-04 00:00:00\n",
      "10 2016-05-05 00:00:00\n",
      "11 2016-05-06 00:00:00\n",
      "12 2016-05-07 00:00:00\n",
      "13 2016-05-08 00:00:00\n",
      "14 2016-05-09 00:00:00\n",
      "15 2016-05-10 00:00:00\n",
      "16 2016-05-11 00:00:00\n",
      "17 2016-05-12 00:00:00\n",
      "18 2016-05-13 00:00:00\n",
      "19 2016-05-14 00:00:00\n",
      "20 2016-05-15 00:00:00\n",
      "21 2016-05-16 00:00:00\n",
      "22 2016-05-17 00:00:00\n",
      "23 2016-05-18 00:00:00\n",
      "24 2016-05-19 00:00:00\n",
      "25 2016-05-20 00:00:00\n",
      "26 2016-05-21 00:00:00\n",
      "27 2016-05-22 00:00:00\n",
      "2 1.018 0.3333333333333333\n",
      "CPU times: user 2h 17min 51s, sys: 48.2 s, total: 2h 18min 39s\n",
      "Wall time: 1h 55min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "alphas = [1.028, 1.023, 1.018]\n",
    "weights = [1/len(alphas)]*len(alphas)\n",
    "sub = 0.\n",
    "\n",
    "for icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n",
    "\n",
    "    te = pre_process(False)\n",
    "    cols = [f\"F{i}\" for i in range(1,29)]\n",
    "\n",
    "    for tdelta in range(0, 28):\n",
    "        day = fday + timedelta(days=tdelta)\n",
    "        print(tdelta, day)\n",
    "        tst = te[(te.date >= day - timedelta(days=max_lags)) & (te.date <= day)].copy()\n",
    "        build_features(tst)\n",
    "        tst = tst.loc[tst.date == day , train_cols]\n",
    "        te.loc[te.date == day, \"sales\"] = alpha*m_lgb.predict(tst) # magic multiplier by kyakovlev\n",
    "\n",
    "\n",
    "\n",
    "    te_sub = te.loc[te.date >= fday, [\"id\", \"sales\"]].copy()\n",
    "#     te_sub.loc[te.date >= fday+ timedelta(days=h), \"id\"] = te_sub.loc[te.date >= fday+timedelta(days=h), \n",
    "#                                                                           \"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n",
    "    te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n",
    "    te_sub.fillna(0., inplace = True)\n",
    "    te_sub.sort_values(\"id\", inplace = True)\n",
    "    te_sub.reset_index(drop=True, inplace = True)\n",
    "    te_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n",
    "    if icount == 0 :\n",
    "        sub = te_sub\n",
    "        sub[cols] *= weight\n",
    "    else:\n",
    "        sub[cols] += te_sub[cols]*weight\n",
    "    print(icount, alpha, weight)\n",
    "\n",
    "\n",
    "sub2 = sub.copy()\n",
    "sub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\n",
    "sub = pd.concat([sub, sub2], axis=0, sort=False)\n",
    "sub.to_csv(\"submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
